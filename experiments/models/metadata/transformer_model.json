{
    "model_name": "transformer_causal_d1",
    "model_type": "Transformer",
    "model_config": {
        "vocab_size": 5,
        "d_model": 256,
        "n_heads": 1,
        "dim_ff": 512,
        "n_layers": 2,
        "n_classes": 2,
        "max_seq_len": 18
    },
    "wheigts_path": "experiments/models/weights/transformer_causal_d1-1754257553.227839.pth"
}