{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de91efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vpl_extraction.vpl_star import VPLStar\n",
    "from examples.dyck1 import Dyck1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8293ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyck1_grammar = Dyck1()\n",
    "\n",
    "vpl_star = VPLStar(dyck1_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1bbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vpl_star.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8a4029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeAutomata( \n",
      " -- states=[\") ['ε []']\", 'ε []'] \n",
      " -- final_states=['ε []'] \n",
      " -- transitions=['Key(ε, [])', \"Key(), ['ε []'])\"] \n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(str(result.tree_automaton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479dc53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar VPG\n",
      "Variables: q1, q0\n",
      "Start Symbols: q1\n",
      "Rules:\n",
      "q1 -> ε    # epsilon\n",
      "\n",
      "\n",
      "Variable Map:\n",
      ") ['ε []'] -> q0\n",
      "ε [] -> q1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.vpg import vpg_from_tree_automata\n",
    "\n",
    "vpg = vpg_from_tree_automata(result.tree_automaton, dyck1_grammar.alphabet)\n",
    "\n",
    "print(vpg.print_grammar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e04fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ε [] ---> Val q1\n",
      "2. ) ['q1'] ---> Val q0\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for k, v in result.tree_automaton.transitions.items():\n",
    "    i+=1\n",
    "    print(f\"{i}. {k.symbol} {[(vpg.variable_map[str(c)]) for c in k.child_states]} ---> Val {vpg.variable_map[str(v)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81227602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPLStar does not match Dyck1 for the word: ()()\n"
     ]
    }
   ],
   "source": [
    "random_word_count = 10000\n",
    "counterexample = False\n",
    "for _ in range(random_word_count):\n",
    "    word = dyck1_grammar.get_random_word()\n",
    "    \n",
    "    if result.is_accepted(word) != dyck1_grammar.is_accepted(word):\n",
    "        print(f\"VPLStar does not match Dyck1 for the word: {word}\")\n",
    "        counterexample = True\n",
    "        break\n",
    "\n",
    "if not counterexample:\n",
    "    print(\"VPLStar matches Dyck1 for all tested words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d6ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = Dyck1().alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d2288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanpe/miniconda3/envs/TransformerExtraction/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjuancommits\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/juanpe/Study/Master/Tree Automata/learning-cfg/wandb/run-20250902_100351-a3ybal0l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/juancommits/transformer-checker/runs/a3ybal0l' target=\"_blank\">golden-monkey-38</a></strong> to <a href='https://wandb.ai/juancommits/transformer-checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/juancommits/transformer-checker' target=\"_blank\">https://wandb.ai/juancommits/transformer-checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/juancommits/transformer-checker/runs/a3ybal0l' target=\"_blank\">https://wandb.ai/juancommits/transformer-checker/runs/a3ybal0l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from experiments.transformer_checker_wrapper import TransformerWrapper\n",
    "\n",
    "transformer = TransformerWrapper(\n",
    "    metadata_path='./experiments/models/metadata/transformer_model.json',\n",
    "    alphabet=alphabet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49862ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vpl_extraction.vpl_star_random_comparator import VPLRandomComparator\n",
    "comparator = VPLRandomComparator(\n",
    "    alphabet=alphabet,\n",
    "    random_trees=500,\n",
    "    max_depth=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9adb90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpl_star = VPLStar(transformer, comparator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f89ee780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6765.01it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6026.30it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4181.76it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2335.36it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8867.45it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 1669.04it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3506.94it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9425.40it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3483.64it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 1786.33it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2693.84it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1770.50it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3506.94it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2704.26it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3236.35it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4670.72it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2857.16it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3256.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3731.59it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6678.83it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4476.31it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3506.94it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2824.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8065.97it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2723.57it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7584.64it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2702.52it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4036.87it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3833.92it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2603.54it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 1158.65it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 558.05it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4116.10it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4877.10it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3731.59it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4826.59it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2336.66it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2159.79it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3125.41it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3279.36it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4029.11it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2695.57it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2763.05it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2398.12it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3533.53it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3460.65it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1012.14it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4202.71it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3584.88it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8176.03it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1926.64it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2661.36it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3650.40it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7639.90it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8004.40it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5745.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5645.09it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2974.68it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6364.65it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2262.30it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 560.81it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1691.93it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5412.01it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7928.74it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2763.05it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 459.55it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5295.84it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6452.78it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4202.71it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3449.26it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5005.14it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3509.88it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5370.43it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5461.33it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4905.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4911.36it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2652.94it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 699.75it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3663.15it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 104.67it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5874.38it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7037.42it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7681.88it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4539.29it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4782.56it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8388.61it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2475.98it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6932.73it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3486.54it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2075.36it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9642.08it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7884.03it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8683.86it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5468.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1865.79it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4832.15it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9362.29it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7307.15it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2824.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5412.01it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4029.11it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3692.17it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2777.68it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2178.86it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3223.91it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3077.26it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7194.35it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1901.32it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4068.19it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3415.56it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4524.60it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3650.40it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3715.06it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2347.12it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 12372.58it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8559.80it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 885.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2053.01it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6668.21it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5857.97it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 487.31it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 292.47it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7133.17it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2697.30it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7194.35it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5745.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 16912.52it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6026.30it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2245.34it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4788.02it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3548.48it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10131.17it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4718.00it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6316.72it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4832.15it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 199.60it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2445.66it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11125.47it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2238.16it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2814.97it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5384.22it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5398.07it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5983.32it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4733.98it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2673.23it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5614.86it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3153.61it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7423.55it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3377.06it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6797.90it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 1925.76it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8081.51it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7096.96it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4405.78it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3923.58it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 366.31it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 1097.41it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 12710.01it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7989.15it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2647.92it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3572.66it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3134.76it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7530.17it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 361.27it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1700.16it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6297.75it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4169.29it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2723.57it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3153.61it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3331.46it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8630.26it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 844.09it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5890.88it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5777.28it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4219.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3816.47it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3334.10it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4429.04it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3659.95it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3813.00it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4324.02it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5555.37it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3890.82it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3039.35it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10082.46it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 699.75it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 1379.25it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 187.93it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2976.79it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3964.37it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3218.96it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6808.94it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4744.69it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6898.53it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3622.02it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5102.56it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3802.63it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3923.58it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3905.31it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5940.94it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8144.28it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7307.15it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4993.22it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2189.09it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 634.16it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 718.82it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4052.47it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 559.91it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8388.61it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7810.62it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3134.76it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2993.79it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4013.69it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1658.48it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2256.22it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4219.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6584.46it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7884.03it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7476.48it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5622.39it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4899.89it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1718.27it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 483.55it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4215.38it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2432.89it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2262.30it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4691.62it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 8405.42it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2763.05it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5053.38it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7928.74it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2462.89it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 724.28it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4485.89it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 9078.58it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3415.56it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2872.81it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10330.80it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3612.66it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7194.35it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4165.15it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 240.57it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 5152.71it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3548.48it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6052.39it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7281.78it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4382.76it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4410.41it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5548.02it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3581.81it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3184.74it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4999.17it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5412.01it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3833.92it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3847.99it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7810.62it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1658.48it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5714.31it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 130.53it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 269.59it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 11397.57it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 1742.54it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5555.37it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 2974.68it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4346.43it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 5526.09it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 3847.99it/s]\n",
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 2824.45it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4826.59it/s]\n"
     ]
    }
   ],
   "source": [
    "res = vpl_star.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4c474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeAutomata( \n",
      " -- states=['ε []', \") ['ε []']\"] \n",
      " -- final_states=['ε []'] \n",
      " -- transitions=['Key((, [\\'ε []\\', \") [\\'ε []\\']\"])', 'Key(ε, [])', \"Key(), ['ε []'])\"] \n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(res.tree_automaton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29522069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ( ['q1', 'q0'] ---> Val q1\n",
      "2. ε [] ---> Val q1\n",
      "3. ) ['q1'] ---> Val q0\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for k, v in res.tree_automaton.transitions.items():\n",
    "    i+=1\n",
    "    print(f\"{i}. {k.symbol} {[(vpg.variable_map[str(c)]) for c in k.child_states]} ---> Val {vpg.variable_map[str(v)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca989023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lengths: 100%|██████████| 1/1 [00:00<00:00, 1390.22it/s]\n",
      "Tokenizing strings: 100%|██████████| 1/1 [00:00<00:00, 4481.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.is_accepted(\"(())\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b0c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar VPG\n",
      "Variables: q1, q0\n",
      "Start Symbols: q0\n",
      "Rules:\n",
      "q0 -> ε    # epsilon\n",
      "q0 -> ( q0 ) q0    # push-pop\n",
      "\n",
      "\n",
      "Variable Map:\n",
      "ε [] -> q0\n",
      ") ['ε []'] -> q1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vpg = vpg_from_tree_automata(res.tree_automaton, alphabet)\n",
    "\n",
    "print(vpg.print_grammar())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformerExtraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
